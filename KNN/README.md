**K Nearest Neighbors :**

Sklearn.neighbors offers tools for learning approaches. Based on unsupervised and supervised neighbors. Uncontrolled nearest neighbors are the basis of many other methods of learning, especially multiple learning, and spectral clustering. Supervised neighbor-based learning comes in two flavors: classification with discrete labels for data, and continuous label regression for data. The idea behind nearest neighboring methods is to locate and predict the mark from these, a predefined number of training samples closest to the new point in space. The sample number may be a user-defined constant (k-nearest neighbor learning) or can differ based on local point density (radius-dependent neighbor learning).


Some **Advantages** of KNN:
1) Rapid calculation time
2) Simple algorithm : to understand
3) Versatile : suitable for regression and classification
4) High accuracy : comparisio with better-supervised learning models is not needed
5) No assumptions about data : No need to make additional assumptions, tune several parameters or create a model. Which makes it crucial in the case of nonlinear results.

Some **Disadvantages** of KNN
1) Accuracy changes with the quality of the data
2) The prediction phase might be slow with large data
3) Sensitive to the scale of the data and inappropriate features
4) Require high memory for store all of the training data
5) It can be computationally expensive as it stores all of the training.
